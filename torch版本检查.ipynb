{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26667b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- å½“å‰ç¯å¢ƒæ ¸å¿ƒå‚æ•° ---\n",
      "Python ç‰ˆæœ¬:  3.11\n",
      "PyTorch ç‰ˆæœ¬: 2.8.0\n",
      "CUDA ç‰ˆæœ¬:    12.6\n",
      "CXX11 ABI:    True\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "def check_env_simple():\n",
    "    # æå– PyTorch å…³è”çš„ CUDA ç‰ˆæœ¬\n",
    "    cuda_ver = torch.version.cuda\n",
    "    # è·å– Python ä¸»ç‰ˆæœ¬å·\n",
    "    py_ver = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "    # è·å– PyTorch ç‰ˆæœ¬ï¼ˆå»é™¤åç¼€ï¼‰\n",
    "    torch_ver = torch.__version__.split('+')[0]\n",
    "    # è·å– ABI çŠ¶æ€\n",
    "    abi_status = torch._C._GLIBCXX_USE_CXX11_ABI\n",
    "\n",
    "    print(\"--- å½“å‰ç¯å¢ƒæ ¸å¿ƒå‚æ•° ---\")\n",
    "    print(f\"Python ç‰ˆæœ¬:  {py_ver}\")\n",
    "    print(f\"PyTorch ç‰ˆæœ¬: {torch_ver}\")\n",
    "    print(f\"CUDA ç‰ˆæœ¬:    {cuda_ver}\")\n",
    "    print(f\"CXX11 ABI:    {abi_status}\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_env_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9128d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-26 14:58:41--  https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp311-cp311-linux_x86_64.whl\n",
      "Resolving github.com (github.com)... 100.65.138.175\n",
      "Connecting to github.com (github.com)|100.65.138.175|:443... connected.\n",
      "HTTP request sent, awaiting response... ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp311-cp311-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf0151c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flash Attention ç‰ˆæœ¬: 2.8.3\n",
      "ğŸš€ ç®—å­è¿è¡Œæµ‹è¯•: æˆåŠŸï¼(Output shape: torch.Size([1, 128, 8, 64]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import flash_attn\n",
    "\n",
    "# 1. æ£€æŸ¥ç‰ˆæœ¬\n",
    "print(\"âœ… Flash Attention ç‰ˆæœ¬:\", flash_attn.__version__)\n",
    "\n",
    "# 2. æ ¸å¿ƒåŠŸèƒ½æ£€æŸ¥ï¼šå°è¯•è°ƒç”¨ç®—å­ï¼ˆéœ€åœ¨ GPU ç¯å¢ƒä¸‹ï¼‰\n",
    "try:\n",
    "    from flash_attn.flash_attn_interface import flash_attn_func\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ® (Batch size 1, Seq len 128, Heads 8, Dim 64)\n",
    "    # Flash Attention è¦æ±‚æ•°æ®ç±»å‹å¿…é¡»æ˜¯ float16 æˆ– bfloat16\n",
    "    q = torch.randn(1, 128, 8, 64, device='cuda', dtype=torch.float16)\n",
    "    k = torch.randn(1, 128, 8, 64, device='cuda', dtype=torch.float16)\n",
    "    v = torch.randn(1, 128, 8, 64, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # è¿è¡Œç®—å­\n",
    "    out = flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=True)\n",
    "    \n",
    "    print(\"ğŸš€ ç®—å­è¿è¡Œæµ‹è¯•: æˆåŠŸï¼(Output shape: {})\".format(out.shape))\n",
    "except Exception as e:\n",
    "    print(\"âŒ ç®—å­è¿è¡Œå¤±è´¥ï¼ŒæŠ¥é”™ä¿¡æ¯:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Causal-Conv1d ç®—å­è¿è¡ŒæˆåŠŸ!\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([2, 16, 64])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨å½“å‰å•å…ƒæ ¼æˆ–ä¸Šä¸€ä¸ªå•å…ƒæ ¼ä¸­æ‰§è¡Œä»£ç æ—¶ Kernel å´©æºƒã€‚\n",
      "\u001b[1;31mè¯·æŸ¥çœ‹å•å…ƒæ ¼ä¸­çš„ä»£ç ï¼Œä»¥ç¡®å®šæ•…éšœçš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må•å‡»<a href='https://aka.ms/vscodeJupyterKernelCrash'>æ­¤å¤„</a>äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n",
      "\u001b[1;31mæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Jupyter <a href='command:jupyter.viewOutput'>log</a>ã€‚"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from causal_conv1d import causal_conv1d_fn\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ® (Batch size, Channels, Length)\n",
    "device = \"cuda\"\n",
    "batch, channels, seqlen = 2, 16, 64\n",
    "x = torch.randn(batch, channels, seqlen, device=device, dtype=torch.bfloat16)\n",
    "weight = torch.randn(channels, 4, device=device, dtype=torch.bfloat16) # å‡è®¾ kernel_size=4\n",
    "bias = torch.randn(channels, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "try:\n",
    "    # è¿è¡Œç®—å­\n",
    "    out = causal_conv1d_fn(x, weight, bias, activation=\"silu\")\n",
    "    print(\"âœ… Causal-Conv1d ç®—å­è¿è¡ŒæˆåŠŸ!\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {out.shape}\") # åº”ä¸º [2, 16, 64]\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¿è¡Œå¤±è´¥ï¼Œé”™è¯¯åŸå› : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c937bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "llamafactory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
