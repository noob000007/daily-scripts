{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310fd641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 18 15:08:28 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:1A:00.0 Off |                  Off |\n",
      "| 30%   42C    P2             155W / 300W |  22297MiB / 24564MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:1B:00.0 Off |                  Off |\n",
      "| 30%   32C    P2              69W / 300W |   8508MiB / 24564MiB |     10%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        On  | 00000000:3D:00.0 Off |                  Off |\n",
      "| 30%   28C    P2              53W / 300W |   1113MiB / 24564MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        On  | 00000000:3E:00.0 Off |                  Off |\n",
      "| 30%   29C    P2              50W / 300W |    723MiB / 24564MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090 D      On  | 00000000:88:00.0 Off |                  Off |\n",
      "| 30%   30C    P8              21W / 300W |     16MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090 D      On  | 00000000:89:00.0 Off |                  Off |\n",
      "| 30%   26C    P8              14W / 300W |     16MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 4090 D      On  | 00000000:B1:00.0 Off |                  Off |\n",
      "| 30%   24C    P8              31W / 300W |     16MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce RTX 4090 D      On  | 00000000:B2:00.0 Off |                  Off |\n",
      "| 30%   25C    P8              13W / 300W |   2551MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                           26MiB |\n",
      "|    0   N/A  N/A      4937      G   /usr/bin/gnome-shell                         12MiB |\n",
      "|    0   N/A  N/A    263575      C   python                                      960MiB |\n",
      "|    0   N/A  N/A    263579      C   python                                      958MiB |\n",
      "|    0   N/A  N/A    263580      C   python                                      958MiB |\n",
      "|    0   N/A  N/A    263581      C   python                                      958MiB |\n",
      "|    0   N/A  N/A    263585      C   python                                      958MiB |\n",
      "|    0   N/A  N/A    263588      C   python                                      958MiB |\n",
      "|    0   N/A  N/A    263591      C   python                                      960MiB |\n",
      "|    0   N/A  N/A    263593      C   python                                      958MiB |\n",
      "|    0   N/A  N/A   1106087      C   /home/fhe/.conda/envs/hf/bin/python       14544MiB |\n",
      "|    1   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A    134945      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    134946      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    134949      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    134951      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    134953      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    134954      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    134958      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    134959      C   python                                      966MiB |\n",
      "|    1   N/A  N/A    256495      C   ...e/zyuan/.conda/envs/scib/bin/python      732MiB |\n",
      "|    2   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A    256495      C   ...e/zyuan/.conda/envs/scib/bin/python      386MiB |\n",
      "|    2   N/A  N/A   2561334      C   python                                      702MiB |\n",
      "|    3   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    3   N/A  N/A   2567781      C   python                                      702MiB |\n",
      "|    4   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    5   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    6   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    7   N/A  N/A      3133      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    7   N/A  N/A   2324105      C   python                                     2532MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d0824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在为 GPU 1 申请 20 GB 显存...\n",
      "GPU 1 申请失败: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 1 has a total capacity of 23.65 GiB of which 15.10 GiB is free. Process 134954 has 966.00 MiB memory in use. Process 134959 has 966.00 MiB memory in use. Process 134946 has 966.00 MiB memory in use. Process 134958 has 966.00 MiB memory in use. Process 134945 has 966.00 MiB memory in use. Process 134949 has 966.00 MiB memory in use. Process 134951 has 966.00 MiB memory in use. Process 134953 has 966.00 MiB memory in use. Process 256495 has 590.00 MiB memory in use. Including non-PyTorch memory, this process has 384.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "正在为 GPU 2 申请 20 GB 显存...\n",
      "正在为 GPU 3 申请 20 GB 显存...\n",
      "正在为 GPU 4 申请 20 GB 显存...\n",
      "GPU 4 申请失败: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 4 has a total capacity of 23.65 GiB of which 19.53 GiB is free. Process 287484 has 478.00 MiB memory in use. Process 287475 has 478.00 MiB memory in use. Process 287463 has 478.00 MiB memory in use. Process 287479 has 478.00 MiB memory in use. Process 287466 has 478.00 MiB memory in use. Process 287455 has 478.00 MiB memory in use. Process 287485 has 478.00 MiB memory in use. Process 287483 has 478.00 MiB memory in use. Including non-PyTorch memory, this process has 346.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "正在为 GPU 5 申请 20 GB 显存...\n",
      "正在为 GPU 6 申请 20 GB 显存...\n",
      "正在为 GPU 7 申请 20 GB 显存...\n",
      "正在为 GPU 8 申请 20 GB 显存...\n",
      "GPU 8 申请失败: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "[成功] 已占用指定显存。按下 Ctrl+C 停止占用并释放。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def occupy_gpu_memory(gpu_ids, target_gb_per_card):\n",
    "    \"\"\"\n",
    "    在指定的 GPU 上占用固定大小的显存\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "    \n",
    "    for gpu_id in gpu_ids:\n",
    "        try:\n",
    "            # 切换到目标显卡\n",
    "            device = torch.device(f\"cuda:{gpu_id}\")\n",
    "            \n",
    "            # 计算需要申请的 Tensor 大小 (float32 占用 4 bytes)\n",
    "            # 1GB = 1024 * 1024 * 1024 bytes\n",
    "            total_elements = (target_gb_per_card * 1024 * 1024 * 1024) // 4\n",
    "            \n",
    "            print(f\"正在为 GPU {gpu_id} 申请 {target_gb_per_card} GB 显存...\")\n",
    "            \n",
    "            # 申请显存并搬运到 GPU\n",
    "            dummy_tensor = torch.empty(int(total_elements), dtype=torch.float32, device=device).fill_(0.0)\n",
    "            tensors.append(dummy_tensor)\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU {gpu_id} 申请失败: {e}\")\n",
    "\n",
    "    print(\"\\n[成功] 已占用指定显存。按下 Ctrl+C 停止占用并释放。\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n正在释放显存并退出...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定要占用的显卡 ID\n",
    "    target_gpus = [1,2,3,4,5,6,7,8]\n",
    "    \n",
    "    # 指定每张卡占用的 GB 数 (根据你 3090/4090 24GB 的情况，建议留出 4-6GB 给系统和基础开销)\n",
    "    # 比如占用 18GB\n",
    "    gb_to_occupy = 20\n",
    "    \n",
    "    occupy_gpu_memory(target_gpus, gb_to_occupy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
